---
title: "Internal report"
author: "Saurav Chowdhury, Sirine Chahma, Reiko Okamoto, Tani Barasch"
date: "17/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and purpose

- Describe the UBC MDS program and the capstone project
- Explain the project objective and how it aligns with the needs of the organization
- Describe the aim of the report: help individuals navigate our GitHub repository, summarize our findings, report performance of selected models, provide suggestions on how to move forward 

## Description of the data

- _Should these visualizations be made using the old dataset or the new one?_
- Discuss notable findings that influenced modeling decisions
- Describe the distribution of the target variable (include plot)
- Demonstrate the data sparsity (include plot)
  - Which columns contain mostly zeros?
- Illustrate the prevalence of missing values in the explanatory variables (include plot)
- Visualize the presence of missing values in the target variable (include plot)
  - Across the playgrounds
  - Across the months (this will justify why we excluded January 2018 from our analysis)
    - Other seasonal patterns worth mentioning
- Mention duplication of information sources
  - "Sex by Marital Status", "Sex by Age", "Sex by School Enrollment"
  - Column A is the sum of columns B, C and D
- Exemplify that some columns are in fact linear transformations of other columns
  - OSMnx columns: proportions versus counts - one is just a normalized version of the other 

## Rationale behind the output

- Describe what kinds of outputs were considered
  - We considered building a model that outputs either a confidence interval or probability distribution
  - Since the distribution of the target variable is skewed, we thought that these kinds of estimates would be more robust than a single-value prediction
- Explain the ultimate decision
  - We respected the fact that playground owners and managers are more comfortable working with single-value predictions
  - We built models that predicted either the mean or median session count
    - Quantile regression was pursued because the median is less sensitive to extreme values than the mean
  - Their performance was evaluated using the root mean square error (RMSE) and mean absolute error (MAE), respectively

## Rationale behind the data split

- We initially received a dataset which included 24 monthly observations for 2506 Biba-enabled playgrounds in the U.S.
- Dates ranged from January 2018 to December 2019
- Observations, with the exception of those from the last three months, were used for model training
- The latter observations were set aside and used as the test set
  - Explain why observations from January 2018 were excluded from our analysis (point to visualization in previous section)
    - Most playgrounds are missing the target value for this month
  - The training set consisted of observations from February 2018 through June 2019 
  - The validation set included observations from July 2019 through September 2019
- This split allowed us to avoid data leakage when pursuing a time-dependent model 

## Analysis with old dataset

- Data (point to the .csv file in the GitHub repository and Google Drive)
- Preprocessing
  - Drop rows missing the target value; impute other missing values with zeros
  - Point to the .py file that contains the relevant functions
- Modeling
  - Linear regression, Ridge, Lasso; SVM; random forest; different flavours of gradient boosting
  - Computation time
  - Fitting a model to data in which the dimensionality was reduced via PCA
  - Fitting a model to data in which the "super playgrounds" were removed 
  - Point to the .ipynb files that contain the relevant work

## Analysis with new dataset

- Data (point to the .csv file in the GitHub repository and Google Drive)
  - Describe the difference between the new and old dataset
    - Explain as to how and why the target variable was capped at 4,000
- Preprocessing
  - Describe the difference between the new and old techniques
    - Imputation
      - For some features, missing values were in fact synonymous with zeros
      - For others, it made more sense to replace missing values with the mean or a specific value (i.e. featured related to election results in Alaska)
    - Feature engineering and selection to reduce model complexity
      - Drop columns in which the proportion of missing values was high
      - Remove correlated features
      - Combine columns using domain knowledge
  - Point to the .py files that contain the relevant functions
- Modeling 
  - Mention that models were trained and optimized on Amazon EC2
  - Tree-based models: random forest, XGBoost, LightGBM, CatBoost
  - Time series approach
  - Mixed model
  
  
  As not all the playgrounds seemed to behave the same we (for example a playground in South of the US may have more visits in December than in summer because winter is not very cold, while summer is very hot), but there where not enough data per playground and to many playgrounds to run a different model for each playground, we decided to implement mixed linear models. The way we did it is that we clustered the playgrounds using some similarities (explained latter) and then the model used fit a regression hyperplan on the whole data. Once we have this hyperplan, for each different hyperplan, the model adds a constant to it, so that there is one regression hyperplan per cluster. By doing it this way, the regression plan of one cluster depends on the regression plan of another cluster, but each cluster still have it's own equation, so the model is less geenric, more specific to a given playground.
  
  We tried to do it in both R (using `lmer` function) and Python (`smf` function from the `statsmodels.formula.api` library)
  
  In R,we tried to cluster the data using the `state`, the `climate`, the `density_class` and the `income_class`. No improvement compared to before, validation RMSE around 200 and validation MAE around 100. We also tried to run K-means with 2 clusters and 4 clusters, and use those clusters when we ran the mixed effects model. In this case, we have to highlight that a same playground may have values in different months. We decided to use K-means just to see if the results were significantly better, and digg more into this problem if this was the case. Unfortunately, this didn't improve neither our RMSE (200) nor our MAE (100), so we didn't digg more into this.
  
  Python : https://github.com/Z2hMedia/capstone_machine_learning/blob/master/src/training_mixed_effects_Python.ipynb 
  Runing Mixed effects model on Python was more of a trouble, because in contrary to R, the function in Python doesn't have an arugment that allows you to drop the columns that make the algorithm not converging. First thing we had to do was to write a function that drops all those columns (see chunk of code starting by #Find the columns that make the fit function fail). Then, we fitted mixed effects models using different clusters (as we did in R). The different clusters that we used were defined by the `climate`, the `density_class` and the `income_class` features (all separated in different models). We got no improvement here either. We got validation RMSE around 200 and validation MAE around 100. We then tried to capped the values at 0, so that we won't have any predicted value that would be negative (as it would make no sense). However, this didn't have much of an impact on the RMSE not on the MAE.
  
  
  - Tiered model
  - Point to the .ipynb files that contain the relevant work
  - Highlight the odd trend in our residuals (include residual plots of different models)

## Final product

- Link to README with usage instructions
- Description of selected models (hyperparameters)
- Rationale behind why models that predict the median session counts were included

Created several scripts to allow you to reproduce our analysis. 
If you want to generate the report from scratch, with all the MAE computed ‘in real life’, using just the raw data as an input, you can just run the command `make report` in the command line from the root of the repo (look at the instructions [here]()). This will run several scripts under the hood. 
The first script `01_split_data.py` is going to take the raw data called `playground_stats` that you will have to place in the `data` folder. It will then split the data between the train and test set as explained before. 
Then, the second script `02_preprocessing.py` will fit a preprocessor on the train data, save it as a `.joblib` files (`imputer.joblib` and `ohe.joblib`), and transform the train and the test sets, and save those 2 preprocessed files (`processed_train` and `processed_test` in the `data` folder). It will also generate 2 dummy preprocessed train sets and test sets, that are smaller versions of the actual train set and test set, that we will use to test our scripts. 
The preprocess script that we decided to use is exactly the same as the one we used on the second dataset we got (see above for more details). 
Regarding the models to run in our pipeline, we selected 3 models : LightGBM, Catboost and XGBoost. The hyperparameters are hard encoded in our scripts, and we performed hyperparameter tuning for LightGBM, Catboost and XGBoost in the corresponding Jupyter Notebook files. We chose those models because they are able to minimise the MAE, they are pretty fast, and they correspond to the best results we got. 3 scripts, one to fit and predict one model.  
Note about Catboost : when you run the pipeline, you’ll see that ‘catboost model has been trained!’ is printed twice, the first time corresponds to the test of the `main` function, while the second one actually correspond to when catboost has been fitted. You can understand that the first time it is printed corresponds roughly to the beginning of the fitting of the catboost model, while the second corresponds to when the model is fitted. 
The last script of this pipeline is the one that generates this report, where all the values of the MAE have not been hard encoded. 

If you want to predict the `unacast_session_count` for a new dataset, this is possible. We created a second pipeline to allow you to do so (see the instruction to run it in the README).
The preprocessing realized on the data and the models that we use are the same as in the report pipeline. The difference with the pipeline described previously is that instead of generating a report, the output of this pipeline is going to be a csv file, save in the `results` folder. This csv will contain all the information that are in the dataset as an input, and extra columns containing the predicted values for each one of the models (LGBM, Catboost and XGBoost) will be added. This csv file is created in the script `07_prediction`. 


## Limitations and suggestions

- Suggest how to work around high-count target values
- Suggest how to impute missing values
- Recommend revisiting columns that were dropped
- Discuss the potential benefits of performing further feature engineering and selection
- Discuss the limitations of our data product

## Conclusion

- Reexplain how these findings add value to the organization
- Highlight key areas for improvement
- Mention how the ongoing pandemic affects our project

## Acknowledgements

- We would like to thank Biba Ventures Inc. for sharing their resources and expertise providing unparalleled support over the course of this project
- Our mentor, Vincenzo Coia
- The UBC MDS program