{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download libraries\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_columns_df(col, key, val):\n",
    "    \"\"\"\n",
    "    This functions takes a dataframe column which is in the\n",
    "    form of list of dictionaries and creates a dataframe\n",
    "    from the keys of the in the inner list of dictionaries \n",
    "    e.g. \"[{'key': A, 'val': 1}, {'key': B, 'val': 2}]\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------------\n",
    "    col : DataFrame Series, the columns whose values are the in the format\n",
    "    of a list of dictionaries.\n",
    "    \n",
    "    key : the keys in the inner dictionary from which column names are to be extracted\n",
    "    \n",
    "    val : the keys in the inner dictionary from which values in the column needs to\n",
    "    be extracted\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ----------------\n",
    "    DataFrame\n",
    "        With the new columns created from the keys of the inner dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    key_list = set()\n",
    "    i=0\n",
    "    # getting all the new column names\n",
    "    while i < len(col):\n",
    "        if type(col[i]) != float:\n",
    "            dic_list = eval(col[i]) #converting col value from string to list\n",
    "            for dic in range(len(dic_list)):\n",
    "                if re.match('[a-zA-Z]', dic_list[dic][str(key)][0]): #removing spanish names\n",
    "                    key_list.add(\"monthly_\"+dic_list[dic][str(key)])\n",
    "        i+=1\n",
    "    \n",
    "    all_cols_dict = defaultdict(list)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(col):\n",
    "        if type(col[i]) != float:\n",
    "            dic_list = eval(col[i]) #converting col value from string to list\n",
    "\n",
    "            for col_names in list(key_list):\n",
    "                flag = 0 #to check if a column name exists in the dictionary\n",
    "                for dic in range(len(dic_list)):\n",
    "                    if dic_list[dic][str(key)] == col_names[8:]: #getting values from the inner dictionary matching the key\n",
    "                        all_cols_dict[col_names].append(dic_list[dic][str(val)]) #putting inner dict values to new default dict\n",
    "                        flag = 1\n",
    "                        break\n",
    "                \n",
    "                if flag==0:\n",
    "                    all_cols_dict[col_names].append(None)\n",
    "\n",
    "        else:\n",
    "            for col_names in list(key_list):\n",
    "                all_cols_dict[col_names].append(None)\n",
    "\n",
    "        i+=1\n",
    "    new_cols_df = pd.DataFrame(all_cols_dict)\n",
    "    \n",
    "    # checking new df has same number of columns as given column\n",
    "    if new_cols_df.shape[0] == col.shape[0]:\n",
    "        return new_cols_df\n",
    "    else:\n",
    "        print(\"Column dimensions don't match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biba_pp(full_data):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the pre-processing of the columns for the biba data\n",
    "    \n",
    "    Paramters\n",
    "    ---------------\n",
    "    \n",
    "    full_data : DataFrame, with no operations done on the biba columns\n",
    "    \n",
    "    Returns\n",
    "    ---------------\n",
    "    DataFrame\n",
    "        with processed biba columns\n",
    "    \n",
    "    \"\"\"\n",
    "    biba_games_df = pd.DataFrame()\n",
    "    biba_games_df = pd.concat([full_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'],\n",
    "                               full_data.loc[:, 'historic_number_of_sessions':'historic_snow']], axis = 1)\n",
    "    \n",
    "    #extracting categorical features\n",
    "    categorical_features = biba_games_df.loc[:, biba_games_df.dtypes == \"object\"]\n",
    "     \n",
    "    # creating cols from list of dictionaries\n",
    "    monthly_survey_df = dict_to_columns_df(categorical_features['monthly_survey'], 'question', 'avg_answer')\n",
    "    monthly_weekday_counts_df = dict_to_columns_df(categorical_features['monthly_weekday_counts'], 'weekday', 'count')\n",
    "    \n",
    "    biba_games_df = pd.concat([biba_games_df, monthly_survey_df, monthly_weekday_counts_df], axis = 1)\n",
    "    \n",
    "    #dropping categorical features\n",
    "    biba_games_df = biba_games_df.drop(columns = list(categorical_features.columns))\n",
    "    \n",
    "    #dropping historic hours with low fill rate\n",
    "    numerical_cols_to_remove = ['historic_hour_0', 'historic_hour_23', 'historic_hour_22', 'historic_hour_21',\n",
    "                                'historic_hour_7','historic_hour_6','historic_hour_5','historic_hour_4', \n",
    "                                'historic_hour_3','historic_hour_2','historic_hour_1', 'MonthYear']\n",
    "    \n",
    "    biba_games_df = biba_games_df.drop(columns = numerical_cols_to_remove)\n",
    "    \n",
    "    impute_biba_games_df =  biba_games_df.fillna(0)\n",
    "    \n",
    "    #removing the previous columns in the input data\n",
    "    cols_to_drop = list(df.loc[:, 'monthly_number_of_sessions': 'distance_to_nearest_bus_stop'].columns) +\\\n",
    "                    list(df.loc[:, 'historic_number_of_sessions' : 'historic_snow'].columns)\n",
    "    \n",
    "    \n",
    "    full_data = full_data.drop(columns = cols_to_drop)\n",
    "    \n",
    "    #adding processed columns\n",
    "    full_data = pd.concat([full_data, impute_biba_games_df], axis = 1)\n",
    "    \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_neighbour(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, preprocess the columns\n",
    "    related to locale information (`city` to\n",
    "    `houses_per_sq_km`). Drop columns with >30%\n",
    "    NaN values and replace remaining NaN values with 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_neighbour = input_data.loc[:, 'city':'houses_per_sq_km']\n",
    "    df_neighbour.drop(columns=['climate'])\n",
    "    missing = df_neighbour.isna()\n",
    "    \n",
    "    # Count number of missing values for each column\n",
    "    num_missing = missing.sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate proportion of missing values for each column\n",
    "    prop_missing = num_missing / df.shape[0]\n",
    "    \n",
    "    # Create a list of columns with >30% of values missing\n",
    "    to_drop = prop_missing[prop_missing > 0.3].index.to_list()\n",
    "    \n",
    "    # Add `country` to the list since all playgrounds are in the U.S.\n",
    "    # Add `city` and `county` since lat. and long. should take care of them\n",
    "    to_drop.append('country')\n",
    "    to_drop.append('city')\n",
    "    to_drop.append('county')\n",
    "    \n",
    "    # Drop columns with names in list\n",
    "    output_data = input_data.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Fill in remaining NaN values in locale-related columns with 0\n",
    "    to_impute = prop_missing[(0 < prop_missing) & (prop_missing <= 0.3)].index.to_list()\n",
    "    to_impute.remove('city')\n",
    "    to_impute.remove('county')\n",
    "    output_data[to_impute] = output_data[to_impute].fillna(0)\n",
    "    output_data['climate'] = input_data['climate']\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, preprocess the columns\n",
    "    related to weather information (`Democrats_08_Votes` to\n",
    "    the end + `climate`). Impute NaN of `Number_of_holidays` \n",
    "    by using the values the we have for the same month,\n",
    "    impute NaN of `Green_2016` by using values found online, or 0, \n",
    "    and replace remaining NaN values with 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_weather = input_data.loc[:, 'Democrats_08_Votes':]\n",
    "    df_weather['state'] = input_data['state']\n",
    "    df_weather['climate'] = input_data['climate']\n",
    "    df_weather['external_id'] = input_data['external_id']\n",
    "    df_weather['month'] = input_data['month']\n",
    "    df_weather['year'] = input_data['year']\n",
    "    \n",
    "    \n",
    "    #fill up NaNs for `Number_of_holidays` column\n",
    "    #I sorted the values so that the values are ordered by time, and the NaNs are at the end of each time period\n",
    "    df_weather = df_weather.sort_values(['month', 'year', 'Number_of_holidays'])\n",
    "    df_weather['Number_of_holidays'] = df_weather['Number_of_holidays'].fillna(method='ffill')\n",
    "    \n",
    "    #fill up NaNs for the `Green_2016` column\n",
    "    #I only found values for Alaska and North Carolina, so I just put 0 for the other states\n",
    "    df_weather['Green_2016'] = np.where(\n",
    "     df_weather['state'] == 'Alaska', 5735, \n",
    "         np.where(\n",
    "            df_weather['state'] == 'North Carolina', 12105,  \n",
    "             np.where(\n",
    "                df_weather['Green_2016'].isnull(), 0, df_weather['Green_2016'] \n",
    "             )\n",
    "         )\n",
    "    )\n",
    "    \n",
    "    df_weather['climate'] = df_weather['climate'].fillna(df_weather['climate'].mode()[0])\n",
    "    \n",
    "    #Substitute every remaining NaNs by 0\n",
    "    df_weather = df_weather.fillna(value=0)\n",
    "    \n",
    "    output_data = input_data.copy()\n",
    "    output_data.loc[:, 'Democrats_08_Votes':] = df_weather.loc[:, 'Democrats_08_Votes':]\n",
    "    output_data['climate'] = df_weather['climate']\n",
    "    \n",
    "    #Tests\n",
    "    \n",
    "    #Check that there are no missing values in the `Number_of_holidays` column\n",
    "    if not output_data['Number_of_holidays'].isnull().sum() == 0:\n",
    "        raise Error('There should not be NaNs in the Number_of_holidays column')\n",
    "    \n",
    "    #Check that every month has only one value for the `Number_of_holiday` column\n",
    "    number_of_error = 0\n",
    "    for month in range(12):\n",
    "        for year in [2018, 2019]:\n",
    "            sub_df = output_data[(output_data['month'] == month+1) & (output_data['year'] == year)]\n",
    "            if len(sub_df['Number_of_holidays'].unique()) > 1:\n",
    "                number_of_error += 1 \n",
    "    if not number_of_error == 0:\n",
    "        raise Error('Every month should have the same value for Number_of_holidays')\n",
    "    \n",
    "    \n",
    "               \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [page](https://en.wikipedia.org/wiki/2016_United_States_presidential_election_in_North_Carolina) is where I found the value for North Carolina, and [this](https://www.nytimes.com/elections/2016/results/alaska) is where I found the results for Alaska."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_categorical(input_data, to_drop=['income_class', 'density_class', 'climate']):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, uses One-Hot-Encoding to encode the categorical variables\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    to_drop : list\n",
    "        The list of the categorical variables on which we want to apply OHE\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output_data = input_data.copy()\n",
    "\n",
    "    #Apply One-Hot-Encoding to each one of the categorical variable\n",
    "    for col in to_drop:\n",
    "        ohe = OneHotEncoder(sparse=False, dtype=int)\n",
    "        sub_df = pd.DataFrame(ohe.fit_transform(input_data[[col]]), columns=ohe.categories_[0])\n",
    "        output_data = pd.concat((output_data, sub_df), axis=1)\n",
    "    #Drop the columns for which we used OHE\n",
    "    output_data.drop(columns = to_drop, inplace=True)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data\n",
    "df = pd.read_csv('../data/train_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_0 = preprocess_neighbour(df) \n",
    "clean_df_1 = biba_pp(clean_df_0)\n",
    "clean_df_2 = preprocess_weather(clean_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50120, 815)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['external_id', 'state', 'income_class', 'density_class', 'climate'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find categorical features\n",
    "clean_df_2.loc[:, clean_df_2.dtypes == \"object\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50120, 821)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = clean_categorical(clean_df_2)\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
