{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_columns', 50)\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>monthly_number_of_sessions</th>\n",
       "      <th>monthly_unique_sessions</th>\n",
       "      <th>monthly_repeated_sessions</th>\n",
       "      <th>monthly_avg_length_of_session</th>\n",
       "      <th>monthly_avg_light_activity</th>\n",
       "      <th>monthly_avg_moderate_activity</th>\n",
       "      <th>monthly_avg_vigorous_activity</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_wind_9_10</th>\n",
       "      <th>avg_wind_10_11</th>\n",
       "      <th>avg_wind_11_12</th>\n",
       "      <th>avg_wind_12_above</th>\n",
       "      <th>perfect_days</th>\n",
       "      <th>unacast_session_count</th>\n",
       "      <th>hpi</th>\n",
       "      <th>state_and_local_amount_per_capita</th>\n",
       "      <th>state_amount_per_capita</th>\n",
       "      <th>local_amount_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900203</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>323.61</td>\n",
       "      <td>0.132207</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.113688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900203</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>323.61</td>\n",
       "      <td>0.132207</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.113688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900203</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>323.61</td>\n",
       "      <td>0.132207</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.113688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>110.38</td>\n",
       "      <td>0.076247</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.064281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>110.38</td>\n",
       "      <td>0.076247</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.064281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_id  month  year  monthly_number_of_sessions  \\\n",
       "0     1900203      3  2019                           0   \n",
       "1     1900203      6  2018                           0   \n",
       "2     1900203      8  2018                           0   \n",
       "3  MR00101775      1  2019                           0   \n",
       "4  MR00101775      8  2019                           0   \n",
       "\n",
       "   monthly_unique_sessions  monthly_repeated_sessions  \\\n",
       "0                        0                          0   \n",
       "1                        0                          0   \n",
       "2                        0                          0   \n",
       "3                        0                          0   \n",
       "4                        0                          0   \n",
       "\n",
       "   monthly_avg_length_of_session  monthly_avg_light_activity  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   monthly_avg_moderate_activity  monthly_avg_vigorous_activity  ...  \\\n",
       "0                            0.0                            0.0  ...   \n",
       "1                            0.0                            0.0  ...   \n",
       "2                            0.0                            0.0  ...   \n",
       "3                            0.0                            0.0  ...   \n",
       "4                            0.0                            0.0  ...   \n",
       "\n",
       "   avg_wind_9_10  avg_wind_10_11  avg_wind_11_12  avg_wind_12_above  \\\n",
       "0            0.0             0.0             0.0                0.0   \n",
       "1            0.0             0.0             0.0                0.0   \n",
       "2            0.0             0.0             0.0                0.0   \n",
       "3            0.0             0.0             0.0                0.0   \n",
       "4            0.0             0.0             0.0                0.0   \n",
       "\n",
       "   perfect_days  unacast_session_count     hpi  \\\n",
       "0           0.0                   78.0  323.61   \n",
       "1           4.0                  111.0  323.61   \n",
       "2           2.0                  110.0  323.61   \n",
       "3           0.0                   10.0  110.38   \n",
       "4           0.0                   11.0  110.38   \n",
       "\n",
       "   state_and_local_amount_per_capita  state_amount_per_capita  \\\n",
       "0                           0.132207                 0.018519   \n",
       "1                           0.132207                 0.018519   \n",
       "2                           0.132207                 0.018519   \n",
       "3                           0.076247                 0.011966   \n",
       "4                           0.076247                 0.011966   \n",
       "\n",
       "   local_amount_per_capita  \n",
       "0                 0.113688  \n",
       "1                 0.113688  \n",
       "2                 0.113688  \n",
       "3                 0.064281  \n",
       "4                 0.064281  \n",
       "\n",
       "[5 rows x 861 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply basic preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_columns_df(col, key, val):\n",
    "    \"\"\"\n",
    "    This functions takes a dataframe column which is in the\n",
    "    form of list of dictionaries and creates a dataframe\n",
    "    from the keys of the in the inner list of dictionaries \n",
    "    e.g. \"[{'key': A, 'val': 1}, {'key': B, 'val': 2}]\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------------\n",
    "    col : DataFrame Series, the columns whose values are the in the format\n",
    "    of a list of dictionaries.\n",
    "    \n",
    "    key : the keys in the inner dictionary from which column names are to be extracted\n",
    "    \n",
    "    val : the keys in the inner dictionary from which values in the column needs to\n",
    "    be extracted\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ----------------\n",
    "    DataFrame\n",
    "        With the new columns created from the keys of the inner dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    key_list = set()\n",
    "    i=0\n",
    "    # getting all the new column names\n",
    "    while i < len(col):\n",
    "        if type(col[i]) != float:\n",
    "            dic_list = eval(col[i]) #converting col value from string to list\n",
    "            for dic in range(len(dic_list)):\n",
    "                if re.match('[a-zA-Z]', dic_list[dic][str(key)][0]): #removing spanish names\n",
    "                    key_list.add(\"monthly_\"+dic_list[dic][str(key)])\n",
    "        i+=1\n",
    "    \n",
    "    all_cols_dict = defaultdict(list)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(col):\n",
    "        if type(col[i]) != float:\n",
    "            dic_list = eval(col[i]) #converting col value from string to list\n",
    "\n",
    "            for col_names in list(key_list):\n",
    "                flag = 0 #to check if a column name exists in the dictionary\n",
    "                for dic in range(len(dic_list)):\n",
    "                    if dic_list[dic][str(key)] == col_names[8:]: #getting values from the inner dictionary matching the key\n",
    "                        all_cols_dict[col_names].append(dic_list[dic][str(val)]) #putting inner dict values to new default dict\n",
    "                        flag = 1\n",
    "                        break\n",
    "                \n",
    "                if flag==0:\n",
    "                    all_cols_dict[col_names].append(None)\n",
    "\n",
    "        else:\n",
    "            for col_names in list(key_list):\n",
    "                all_cols_dict[col_names].append(None)\n",
    "\n",
    "        i+=1\n",
    "    new_cols_df = pd.DataFrame(all_cols_dict)\n",
    "    \n",
    "    # checking new df has same number of columns as given column\n",
    "    if new_cols_df.shape[0] == col.shape[0]:\n",
    "        return new_cols_df\n",
    "    else:\n",
    "        print(\"Column dimensions don't match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_biba(full_data):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the pre-processing of the columns for the biba data\n",
    "    \n",
    "    Paramters\n",
    "    ---------------\n",
    "    \n",
    "    full_data : DataFrame, with no operations done on the biba columns\n",
    "    \n",
    "    Returns\n",
    "    ---------------\n",
    "    DataFrame\n",
    "        with processed biba columns\n",
    "    \n",
    "    \"\"\"\n",
    "    biba_games_df = pd.DataFrame()\n",
    "    biba_games_df = pd.concat([full_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'],\n",
    "                               full_data.loc[:, 'historic_number_of_sessions':'historic_snow']], axis = 1)\n",
    "    \n",
    "    #extracting categorical features\n",
    "    categorical_features = biba_games_df.loc[:, biba_games_df.dtypes == \"object\"]\n",
    "     \n",
    "    # creating cols from list of dictionaries\n",
    "    monthly_survey_df = dict_to_columns_df(categorical_features['monthly_survey'], 'question', 'avg_answer')\n",
    "    monthly_weekday_counts_df = dict_to_columns_df(categorical_features['monthly_weekday_counts'], 'weekday', 'count')\n",
    "    \n",
    "    biba_games_df = pd.concat([biba_games_df, monthly_survey_df, monthly_weekday_counts_df], axis = 1)\n",
    "    \n",
    "    #dropping categorical features\n",
    "    biba_games_df = biba_games_df.drop(columns = list(categorical_features.columns))\n",
    "    \n",
    "    #dropping historic hours with low fill rate\n",
    "    numerical_cols_to_remove = ['historic_hour_0', 'historic_hour_23', 'historic_hour_22', 'historic_hour_21',\n",
    "                                'historic_hour_7','historic_hour_6','historic_hour_5','historic_hour_4', \n",
    "                                'historic_hour_3','historic_hour_2','historic_hour_1', 'MonthYear']\n",
    "    \n",
    "    biba_games_df = biba_games_df.drop(columns = numerical_cols_to_remove)\n",
    "    \n",
    "    impute_biba_games_df =  biba_games_df.fillna(0)\n",
    "    \n",
    "    #removing the previous columns in the input data\n",
    "    cols_to_drop = list(df.loc[:, 'monthly_number_of_sessions': 'distance_to_nearest_bus_stop'].columns) +\\\n",
    "                    list(df.loc[:, 'historic_number_of_sessions' : 'historic_snow'].columns)\n",
    "    \n",
    "    \n",
    "    full_data = full_data.drop(columns = cols_to_drop)\n",
    "    \n",
    "    #adding processed columns\n",
    "    full_data = pd.concat([full_data, impute_biba_games_df], axis = 1)\n",
    "    \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation drops columns with survey answers\n",
    "\n",
    "def preprocess_biba_no_survey(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, process the columns related to\n",
    "    Biba Playground Games. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data: pandas.core.frame.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data: pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    # Concatenate relevant columns into a single dataframe \n",
    "    \n",
    "    biba_df = pd.DataFrame()\n",
    "    biba_df = pd.concat([input_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'],\n",
    "                         input_data.loc[:, 'historic_number_of_sessions':'historic_snow']], axis=1)\n",
    "    \n",
    "    \n",
    "    # Extract categorical features\n",
    "    categorical_features = biba_df.loc[:, biba_df.dtypes == \"object\"]\n",
    "    \n",
    "    # Identify categorical features and numerical features with high prop. of NaN values\n",
    "    to_drop = categorical_features.columns.to_list()\n",
    "    \n",
    "    to_drop += ['historic_hour_0', 'historic_hour_23', 'historic_hour_22', 'historic_hour_21',\n",
    "                'historic_hour_7','historic_hour_6','historic_hour_5','historic_hour_4', \n",
    "                'historic_hour_3','historic_hour_2','historic_hour_1', 'MonthYear']\n",
    "    \n",
    "    # Drop said columns\n",
    "    biba_df = biba_df.drop(columns=to_drop)\n",
    "    \n",
    "    # Impute any remaining NaN values with 0\n",
    "    biba_df = biba_df.fillna(0)\n",
    "    \n",
    "    # Remove the old, unprocessed colums in the input data \n",
    "    old_columns = input_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'].columns.to_list() +\\\n",
    "                  input_data.loc[:, 'historic_number_of_sessions':'historic_snow'].columns.to_list()\n",
    "    \n",
    "    input_data = input_data.drop(old_columns)\n",
    "    \n",
    "    # Add preprocessed columns back\n",
    "    \n",
    "    output_data = pd.concat([input_data, biba_df], axis=1)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, preprocess the columns\n",
    "    related to weather information (`Democrats_08_Votes` to\n",
    "    the end + `climate`). Impute NaN of `Number_of_holidays` \n",
    "    by using the values the we have for the same month,\n",
    "    impute NaN of `Green_2016` by using values found online, or 0, \n",
    "    and replace remaining NaN values with 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_weather = input_data.loc[:, 'Democrats_08_Votes':]\n",
    "    df_weather['state'] = input_data['state']\n",
    "    df_weather['climate'] = input_data['climate']\n",
    "    df_weather['external_id'] = input_data['external_id']\n",
    "    df_weather['month'] = input_data['month']\n",
    "    df_weather['year'] = input_data['year']\n",
    "    \n",
    "    \n",
    "    #fill up NaNs for `Number_of_holidays` column\n",
    "    #I sorted the values so that the values are ordered by time, and the NaNs are at the end of each time period\n",
    "    df_weather = df_weather.sort_values(['month', 'year', 'Number_of_holidays'])\n",
    "    df_weather['Number_of_holidays'] = df_weather['Number_of_holidays'].fillna(method='ffill')\n",
    "    \n",
    "    #fill up NaNs for the `Green_2016` column\n",
    "    #I only found values for Alaska and North Carolina, so I just put 0 for the other states\n",
    "    df_weather['Green_2016'] = np.where(\n",
    "     df_weather['state'] == 'Alaska', 5735, \n",
    "         np.where(\n",
    "            df_weather['state'] == 'North Carolina', 12105,  \n",
    "             np.where(\n",
    "                df_weather['Green_2016'].isnull(), 0, df_weather['Green_2016'] \n",
    "             )\n",
    "         )\n",
    "    )\n",
    "    \n",
    "    df_weather['climate'] = df_weather['climate'].fillna(df_weather['climate'].mode()[0])\n",
    "    \n",
    "    #Substitute every remaining NaNs by 0\n",
    "    df_weather = df_weather.fillna(value=0)\n",
    "    \n",
    "    output_data = input_data.copy()\n",
    "    output_data.loc[:, 'Democrats_08_Votes':] = df_weather.loc[:, 'Democrats_08_Votes':]\n",
    "    output_data['climate'] = df_weather['climate']\n",
    "    \n",
    "    #Tests\n",
    "    \n",
    "    #Check that there are no missing values in the `Number_of_holidays` column\n",
    "    if not output_data['Number_of_holidays'].isnull().sum() == 0:\n",
    "        raise Error('There should not be NaNs in the Number_of_holidays column')\n",
    "    \n",
    "    #Check that every month has only one value for the `Number_of_holiday` column\n",
    "    number_of_error = 0\n",
    "    for month in range(12):\n",
    "        for year in [2018, 2019]:\n",
    "            sub_df = output_data[(output_data['month'] == month+1) & (output_data['year'] == year)]\n",
    "            if len(sub_df['Number_of_holidays'].unique()) > 1:\n",
    "                number_of_error += 1 \n",
    "    if not number_of_error == 0:\n",
    "        raise Error('Every month should have the same value for Number_of_holidays')\n",
    "               \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_neighbour(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, preprocess the columns\n",
    "    related to locale information (`city` to\n",
    "    `houses_per_sq_km`). Drop columns with >30%\n",
    "    NaN values and replace remaining NaN values with 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_neighbour = input_data.loc[:, 'city':'houses_per_sq_km']\n",
    "    df_neighbour.drop(columns=['climate'])\n",
    "    missing = df_neighbour.isna()\n",
    "    \n",
    "    # Count number of missing values for each column\n",
    "    num_missing = missing.sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate proportion of missing values for each column\n",
    "    prop_missing = num_missing / df.shape[0]\n",
    "    \n",
    "    # Create a list of columns with >30% of values missing\n",
    "    to_drop = prop_missing[prop_missing > 0.3].index.to_list()\n",
    "    \n",
    "    # Add `country` to the list since all playgrounds are in the U.S.\n",
    "    # Add `city` and `county` since lat. and long. should take care of them\n",
    "    to_drop.append('country')\n",
    "    to_drop.append('city')\n",
    "    to_drop.append('county')\n",
    "    \n",
    "    # Drop columns with names in list\n",
    "    output_data = input_data.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Fill in remaining NaN values in locale-related columns with 0\n",
    "    to_impute = prop_missing[(0 < prop_missing) & (prop_missing <= 0.3)].index.to_list()\n",
    "    to_impute.remove('city')\n",
    "    to_impute.remove('county')\n",
    "    output_data[to_impute] = output_data[to_impute].fillna(0)\n",
    "    output_data['climate'] = input_data['climate']\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_biba(df)\n",
    "data = preprocess_weather(data)\n",
    "data = preprocess_neighbour(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>B20004e10</th>\n",
       "      <th>B11016e1</th>\n",
       "      <th>B12001e12</th>\n",
       "      <th>B20004e11</th>\n",
       "      <th>B19125e1</th>\n",
       "      <th>B12001e13</th>\n",
       "      <th>B23008e22</th>\n",
       "      <th>...</th>\n",
       "      <th>monthly_safety</th>\n",
       "      <th>monthly_variety</th>\n",
       "      <th>monthly_condition</th>\n",
       "      <th>monthly_Wednesday</th>\n",
       "      <th>monthly_Thursday</th>\n",
       "      <th>monthly_Friday</th>\n",
       "      <th>monthly_Saturday</th>\n",
       "      <th>monthly_Monday</th>\n",
       "      <th>monthly_Sunday</th>\n",
       "      <th>monthly_Tuesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900203</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900203</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900203</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_id  month  year  B20004e10  B11016e1  B12001e12  B20004e11  \\\n",
       "0     1900203      3  2019      51111      1868        688          0   \n",
       "1     1900203      6  2018      51111      1868        688          0   \n",
       "2     1900203      8  2018      51111      1868        688          0   \n",
       "3  MR00101775      1  2019      45484      2613        980      30417   \n",
       "4  MR00101775      8  2019      45484      2613        980      30417   \n",
       "\n",
       "   B19125e1  B12001e13  B23008e22  ...  monthly_safety  monthly_variety  \\\n",
       "0     78934       1342          0  ...             0.0              0.0   \n",
       "1     78934       1342          0  ...             0.0              0.0   \n",
       "2     78934       1342          0  ...             0.0              0.0   \n",
       "3     45578       1097         66  ...             0.0              0.0   \n",
       "4     45578       1097         66  ...             0.0              0.0   \n",
       "\n",
       "   monthly_condition  monthly_Wednesday  monthly_Thursday  monthly_Friday  \\\n",
       "0                0.0                0.0               0.0             0.0   \n",
       "1                0.0                0.0               0.0             0.0   \n",
       "2                0.0                0.0               0.0             0.0   \n",
       "3                0.0                0.0               0.0             0.0   \n",
       "4                0.0                0.0               0.0             0.0   \n",
       "\n",
       "   monthly_Saturday  monthly_Monday  monthly_Sunday  monthly_Tuesday  \n",
       "0               0.0             0.0             0.0              0.0  \n",
       "1               0.0             0.0             0.0              0.0  \n",
       "2               0.0             0.0             0.0              0.0  \n",
       "3               0.0             0.0             0.0              0.0  \n",
       "4               0.0             0.0             0.0              0.0  \n",
       "\n",
       "[5 rows x 815 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of ouput data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_categorical(input_data, to_drop=['income_class', 'density_class', 'climate']):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, uses One-Hot-Encoding to encode the categorical variables\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    to_drop : list\n",
    "        The list of the categorical variables on which we want to apply OHE\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output_data = input_data.copy()\n",
    "\n",
    "    #Apply One-Hot-Encoding to each one of the categorical variable\n",
    "    for col in to_drop:\n",
    "        ohe = OneHotEncoder(sparse=False, dtype=int)\n",
    "        sub_df = pd.DataFrame(ohe.fit_transform(input_data[[col]]), columns=ohe.categories_[0])\n",
    "        output_data = pd.concat((output_data, sub_df), axis=1)\n",
    "    #Drop the columns for which we used OHE\n",
    "    output_data.drop(columns = to_drop, inplace=True)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on three columns:\n",
    "data = clean_categorical(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>B20004e10</th>\n",
       "      <th>B11016e1</th>\n",
       "      <th>B12001e12</th>\n",
       "      <th>B20004e11</th>\n",
       "      <th>B19125e1</th>\n",
       "      <th>B12001e13</th>\n",
       "      <th>B23008e22</th>\n",
       "      <th>...</th>\n",
       "      <th>monthly_Tuesday</th>\n",
       "      <th>HI</th>\n",
       "      <th>LI</th>\n",
       "      <th>MI</th>\n",
       "      <th>HD</th>\n",
       "      <th>LD</th>\n",
       "      <th>MD</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900203</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900203</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900203</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 821 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_id  month  year  B20004e10  B11016e1  B12001e12  B20004e11  \\\n",
       "0     1900203      3  2019      51111      1868        688          0   \n",
       "1     1900203      6  2018      51111      1868        688          0   \n",
       "2     1900203      8  2018      51111      1868        688          0   \n",
       "3  MR00101775      1  2019      45484      2613        980      30417   \n",
       "4  MR00101775      8  2019      45484      2613        980      30417   \n",
       "\n",
       "   B19125e1  B12001e13  B23008e22  ...  monthly_Tuesday  HI  LI  MI  HD  LD  \\\n",
       "0     78934       1342          0  ...              0.0   1   0   0   1   0   \n",
       "1     78934       1342          0  ...              0.0   1   0   0   1   0   \n",
       "2     78934       1342          0  ...              0.0   1   0   0   1   0   \n",
       "3     45578       1097         66  ...              0.0   0   1   0   0   1   \n",
       "4     45578       1097         66  ...              0.0   0   1   0   0   1   \n",
       "\n",
       "   MD  A  C  D  \n",
       "0   0  0  1  0  \n",
       "1   0  0  1  0  \n",
       "2   0  0  1  0  \n",
       "3   0  0  1  0  \n",
       "4   0  0  1  0  \n",
       "\n",
       "[5 rows x 821 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe after all pre-processing\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('unacast_session_count', axis=1)\n",
    "y = data.loc[:, 'unacast_session_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, drop `external_id` and `state`\n",
    "X = X.drop(['external_id', 'state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "days_since_first_sess    17800\n",
       "D                            0\n",
       "B12001e2                     0\n",
       "B11005e9                     0\n",
       "B13016e5                     0\n",
       "                         ...  \n",
       "precip_mm_10_above           0\n",
       "precip_mm_1_10               0\n",
       "precip_mm_0_1                0\n",
       "precip_mm_none               0\n",
       "external_id                  0\n",
       "Length: 820, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are missing values in X\n",
    "X.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, fill `days_since_first_sess` with 0\n",
    "X['days_since_first_sess'] = X['days_since_first_sess'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are missing values in y\n",
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `NaN` values in `y` - that's good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40096, 818)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10024, 818)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a `GradientBoostingRegressor` with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'verbose': 1,\n",
    "          'random_state': 2020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      213669.2304            4.86m\n",
      "         2      205282.8246            4.79m\n",
      "         3      198390.9645            4.72m\n",
      "         4      191658.6878            5.01m\n",
      "         5      186589.0004            4.92m\n",
      "         6      181186.5017            4.90m\n",
      "         7      177208.5001            4.84m\n",
      "         8      174242.0719            5.04m\n",
      "         9      171638.3291            5.22m\n",
      "        10      165022.6046            5.32m\n",
      "        20      129466.3909            4.65m\n",
      "        30       99860.7850            3.98m\n",
      "        40       81710.4135            3.35m\n",
      "        50       70320.0408            2.81m\n",
      "        60       62436.7513            2.22m\n",
      "        70       58201.8945            1.67m\n",
      "        80       53252.5016            1.11m\n",
      "        90       49244.3230           33.05s\n",
      "       100       46957.9923            0.00s\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(**params)\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370204.9588049915"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MSE\n",
    "y_pred = gbr.predict(X_valid)\n",
    "mean_squared_error(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27425744561888155"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate R^2 of the prediction\n",
    "gbr.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another `GradientBoostingRegressor` where `n_estimators` is doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1000 = {'verbose': 1,\n",
    "               'n_estimators': 1000,\n",
    "               'random_state': 2020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      213669.2304           48.80m\n",
      "         2      205282.8246           47.63m\n",
      "         3      198390.9645           47.96m\n",
      "         4      191658.6878           49.80m\n",
      "         5      186589.0004           53.81m\n",
      "         6      181186.5017           53.67m\n",
      "         7      177208.5001           53.37m\n",
      "         8      174242.0719           53.11m\n",
      "         9      171638.3291           52.80m\n",
      "        10      165022.6046           52.90m\n",
      "        20      129466.3909           50.69m\n",
      "        30       99860.7850           51.99m\n",
      "        40       81710.4135           51.96m\n",
      "        50       70320.0408           51.28m\n",
      "        60       62436.7513           53.76m\n",
      "        70       58201.8945           53.96m\n",
      "        80       53252.5016           52.57m\n",
      "        90       49244.3230           50.99m\n",
      "       100       46957.9923           49.76m\n",
      "       200       33108.7092           43.56m\n",
      "       300       26537.4012           37.29m\n",
      "       400       22282.0518           32.59m\n",
      "       500       19214.4100           26.89m\n",
      "       600       16629.4686           21.27m\n",
      "       700       14954.9852           15.76m\n",
      "       800       13701.1824           10.43m\n",
      "       900       12449.9097            5.17m\n",
      "      1000       11590.5045            0.00s\n"
     ]
    }
   ],
   "source": [
    "gbr_1000 = GradientBoostingRegressor(**params_1000)\n",
    "\n",
    "t0 = time.time()\n",
    "gbr_1000.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "fit_time = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2861276364206973"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate R^2 of the prediction\n",
    "gbr_1000.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364149.91425749223"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_1000 = gbr_1000.predict(X_valid)\n",
    "mean_squared_error(y_valid, y_pred_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform randomized search of optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'min_samples_split': [2, 4, 6],\n",
    "              'max_depth': [3, 5, 7, 9],\n",
    "              'max_features': ['auto', 'sqrt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=3 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      175382.1518           93.84m\n",
      "         2      167979.2856          100.94m\n",
      "         3      159780.7409           98.65m\n",
      "         4      152920.9069           99.50m\n",
      "         5      148649.0199          100.39m\n",
      "         6      143598.0615          102.15m\n",
      "         7      139962.7636          100.08m\n",
      "         8      137345.5012           98.46m\n",
      "         9      135097.3866           97.04m\n",
      "        10      131214.4945           96.25m\n",
      "        20       98106.9549           91.97m\n",
      "        30       80402.1899           91.15m\n",
      "        40       67393.8868           89.61m\n",
      "        50       59352.0962           88.59m\n",
      "        60       54595.4692           86.59m\n"
     ]
    }
   ],
   "source": [
    "gbr_gs = GradientBoostingRegressor(n_estimators=1000, verbose=1, random_state=2020)\n",
    "\n",
    "rscv = RandomizedSearchCV(gbr_gs, param_grid, n_iter=3, verbose=2, n_jobs=1, random_state=2020)\n",
    "\n",
    "search = rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "- http://www.chengli.io/tutorials/gradient_boosting.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
