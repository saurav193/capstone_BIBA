{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_columns', 50)\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>monthly_number_of_sessions</th>\n",
       "      <th>monthly_unique_sessions</th>\n",
       "      <th>monthly_repeated_sessions</th>\n",
       "      <th>monthly_avg_length_of_session</th>\n",
       "      <th>monthly_avg_light_activity</th>\n",
       "      <th>monthly_avg_moderate_activity</th>\n",
       "      <th>monthly_avg_vigorous_activity</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_wind_9_10</th>\n",
       "      <th>avg_wind_10_11</th>\n",
       "      <th>avg_wind_11_12</th>\n",
       "      <th>avg_wind_12_above</th>\n",
       "      <th>perfect_days</th>\n",
       "      <th>unacast_session_count</th>\n",
       "      <th>hpi</th>\n",
       "      <th>state_and_local_amount_per_capita</th>\n",
       "      <th>state_amount_per_capita</th>\n",
       "      <th>local_amount_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900203</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>323.61</td>\n",
       "      <td>0.132207</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.113688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900203</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>323.61</td>\n",
       "      <td>0.132207</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.113688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900203</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>323.61</td>\n",
       "      <td>0.132207</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.113688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>110.38</td>\n",
       "      <td>0.076247</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.064281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>110.38</td>\n",
       "      <td>0.076247</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.064281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_id  month  year  monthly_number_of_sessions  \\\n",
       "0     1900203      3  2019                           0   \n",
       "1     1900203      6  2018                           0   \n",
       "2     1900203      8  2018                           0   \n",
       "3  MR00101775      1  2019                           0   \n",
       "4  MR00101775      8  2019                           0   \n",
       "\n",
       "   monthly_unique_sessions  monthly_repeated_sessions  \\\n",
       "0                        0                          0   \n",
       "1                        0                          0   \n",
       "2                        0                          0   \n",
       "3                        0                          0   \n",
       "4                        0                          0   \n",
       "\n",
       "   monthly_avg_length_of_session  monthly_avg_light_activity  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   monthly_avg_moderate_activity  monthly_avg_vigorous_activity  ...  \\\n",
       "0                            0.0                            0.0  ...   \n",
       "1                            0.0                            0.0  ...   \n",
       "2                            0.0                            0.0  ...   \n",
       "3                            0.0                            0.0  ...   \n",
       "4                            0.0                            0.0  ...   \n",
       "\n",
       "   avg_wind_9_10  avg_wind_10_11  avg_wind_11_12  avg_wind_12_above  \\\n",
       "0            0.0             0.0             0.0                0.0   \n",
       "1            0.0             0.0             0.0                0.0   \n",
       "2            0.0             0.0             0.0                0.0   \n",
       "3            0.0             0.0             0.0                0.0   \n",
       "4            0.0             0.0             0.0                0.0   \n",
       "\n",
       "   perfect_days  unacast_session_count     hpi  \\\n",
       "0           0.0                   78.0  323.61   \n",
       "1           4.0                  111.0  323.61   \n",
       "2           2.0                  110.0  323.61   \n",
       "3           0.0                   10.0  110.38   \n",
       "4           0.0                   11.0  110.38   \n",
       "\n",
       "   state_and_local_amount_per_capita  state_amount_per_capita  \\\n",
       "0                           0.132207                 0.018519   \n",
       "1                           0.132207                 0.018519   \n",
       "2                           0.132207                 0.018519   \n",
       "3                           0.076247                 0.011966   \n",
       "4                           0.076247                 0.011966   \n",
       "\n",
       "   local_amount_per_capita  \n",
       "0                 0.113688  \n",
       "1                 0.113688  \n",
       "2                 0.113688  \n",
       "3                 0.064281  \n",
       "4                 0.064281  \n",
       "\n",
       "[5 rows x 861 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply basic preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_columns_df(col, key, val):\n",
    "    \"\"\"\n",
    "    This functions takes a dataframe column which is in the\n",
    "    form of list of dictionaries and creates a dataframe\n",
    "    from the keys of the in the inner list of dictionaries \n",
    "    e.g. \"[{'key': A, 'val': 1}, {'key': B, 'val': 2}]\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------------\n",
    "    col : DataFrame Series, the columns whose values are the in the format\n",
    "    of a list of dictionaries.\n",
    "    \n",
    "    key : the keys in the inner dictionary from which column names are to be extracted\n",
    "    \n",
    "    val : the keys in the inner dictionary from which values in the column needs to\n",
    "    be extracted\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ----------------\n",
    "    DataFrame\n",
    "        With the new columns created from the keys of the inner dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    key_list = set()\n",
    "    i=0\n",
    "    # getting all the new column names\n",
    "    while i < len(col):\n",
    "        if type(col[i]) != float:\n",
    "            dic_list = eval(col[i]) #converting col value from string to list\n",
    "            for dic in range(len(dic_list)):\n",
    "                if re.match('[a-zA-Z]', dic_list[dic][str(key)][0]): #removing spanish names\n",
    "                    key_list.add(\"monthly_\"+dic_list[dic][str(key)])\n",
    "        i+=1\n",
    "    \n",
    "    all_cols_dict = defaultdict(list)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(col):\n",
    "        if type(col[i]) != float:\n",
    "            dic_list = eval(col[i]) #converting col value from string to list\n",
    "\n",
    "            for col_names in list(key_list):\n",
    "                flag = 0 #to check if a column name exists in the dictionary\n",
    "                for dic in range(len(dic_list)):\n",
    "                    if dic_list[dic][str(key)] == col_names[8:]: #getting values from the inner dictionary matching the key\n",
    "                        all_cols_dict[col_names].append(dic_list[dic][str(val)]) #putting inner dict values to new default dict\n",
    "                        flag = 1\n",
    "                        break\n",
    "                \n",
    "                if flag==0:\n",
    "                    all_cols_dict[col_names].append(None)\n",
    "\n",
    "        else:\n",
    "            for col_names in list(key_list):\n",
    "                all_cols_dict[col_names].append(None)\n",
    "\n",
    "        i+=1\n",
    "    new_cols_df = pd.DataFrame(all_cols_dict)\n",
    "    \n",
    "    # checking new df has same number of columns as given column\n",
    "    if new_cols_df.shape[0] == col.shape[0]:\n",
    "        return new_cols_df\n",
    "    else:\n",
    "        print(\"Column dimensions don't match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_biba(full_data):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the pre-processing of the columns for the biba data\n",
    "    \n",
    "    Paramters\n",
    "    ---------------\n",
    "    \n",
    "    full_data : DataFrame, with no operations done on the biba columns\n",
    "    \n",
    "    Returns\n",
    "    ---------------\n",
    "    DataFrame\n",
    "        with processed biba columns\n",
    "    \n",
    "    \"\"\"\n",
    "    biba_games_df = pd.DataFrame()\n",
    "    biba_games_df = pd.concat([full_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'],\n",
    "                               full_data.loc[:, 'historic_number_of_sessions':'historic_snow']], axis = 1)\n",
    "    \n",
    "    #extracting categorical features\n",
    "    categorical_features = biba_games_df.loc[:, biba_games_df.dtypes == \"object\"]\n",
    "     \n",
    "    # creating cols from list of dictionaries\n",
    "    monthly_survey_df = dict_to_columns_df(categorical_features['monthly_survey'], 'question', 'avg_answer')\n",
    "    monthly_weekday_counts_df = dict_to_columns_df(categorical_features['monthly_weekday_counts'], 'weekday', 'count')\n",
    "    \n",
    "    biba_games_df = pd.concat([biba_games_df, monthly_survey_df, monthly_weekday_counts_df], axis = 1)\n",
    "    \n",
    "    #dropping categorical features\n",
    "    biba_games_df = biba_games_df.drop(columns = list(categorical_features.columns))\n",
    "    \n",
    "    #dropping historic hours with low fill rate\n",
    "    numerical_cols_to_remove = ['historic_hour_0', 'historic_hour_23', 'historic_hour_22', 'historic_hour_21',\n",
    "                                'historic_hour_7','historic_hour_6','historic_hour_5','historic_hour_4', \n",
    "                                'historic_hour_3','historic_hour_2','historic_hour_1', 'MonthYear']\n",
    "    \n",
    "    biba_games_df = biba_games_df.drop(columns = numerical_cols_to_remove)\n",
    "    \n",
    "    impute_biba_games_df =  biba_games_df.fillna(0)\n",
    "    \n",
    "    #removing the previous columns in the input data\n",
    "    cols_to_drop = list(df.loc[:, 'monthly_number_of_sessions': 'distance_to_nearest_bus_stop'].columns) +\\\n",
    "                    list(df.loc[:, 'historic_number_of_sessions' : 'historic_snow'].columns)\n",
    "    \n",
    "    \n",
    "    full_data = full_data.drop(columns = cols_to_drop)\n",
    "    \n",
    "    #adding processed columns\n",
    "    full_data = pd.concat([full_data, impute_biba_games_df], axis = 1)\n",
    "    \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation drops columns with survey answers\n",
    "\n",
    "def preprocess_biba_no_survey(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, process the columns related to\n",
    "    Biba Playground Games. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data: pandas.core.frame.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data: pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    # Concatenate relevant columns into a single dataframe \n",
    "    \n",
    "    biba_df = pd.DataFrame()\n",
    "    biba_df = pd.concat([input_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'],\n",
    "                         input_data.loc[:, 'historic_number_of_sessions':'historic_snow']], axis=1)\n",
    "    \n",
    "    \n",
    "    # Extract categorical features\n",
    "    categorical_features = biba_df.loc[:, biba_df.dtypes == \"object\"]\n",
    "    \n",
    "    # Identify categorical features and numerical features with high prop. of NaN values\n",
    "    to_drop = categorical_features.columns.to_list()\n",
    "    \n",
    "    to_drop += ['historic_hour_0', 'historic_hour_23', 'historic_hour_22', 'historic_hour_21',\n",
    "                'historic_hour_7','historic_hour_6','historic_hour_5','historic_hour_4', \n",
    "                'historic_hour_3','historic_hour_2','historic_hour_1', 'MonthYear']\n",
    "    \n",
    "    # Drop said columns\n",
    "    biba_df = biba_df.drop(columns=to_drop)\n",
    "    \n",
    "    # Impute any remaining NaN values with 0\n",
    "    biba_df = biba_df.fillna(0)\n",
    "    \n",
    "    # Remove the old, unprocessed colums in the input data \n",
    "    old_columns = input_data.loc[:, 'monthly_number_of_sessions':'distance_to_nearest_bus_stop'].columns.to_list() +\\\n",
    "                  input_data.loc[:, 'historic_number_of_sessions':'historic_snow'].columns.to_list()\n",
    "    \n",
    "    input_data = input_data.drop(old_columns)\n",
    "    \n",
    "    # Add preprocessed columns back\n",
    "    \n",
    "    output_data = pd.concat([input_data, biba_df], axis=1)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, preprocess the columns\n",
    "    related to weather information (`Democrats_08_Votes` to\n",
    "    the end + `climate`). Impute NaN of `Number_of_holidays` \n",
    "    by using the values the we have for the same month,\n",
    "    impute NaN of `Green_2016` by using values found online, or 0, \n",
    "    and replace remaining NaN values with 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_weather = input_data.loc[:, 'Democrats_08_Votes':]\n",
    "    df_weather['state'] = input_data['state']\n",
    "    df_weather['climate'] = input_data['climate']\n",
    "    df_weather['external_id'] = input_data['external_id']\n",
    "    df_weather['month'] = input_data['month']\n",
    "    df_weather['year'] = input_data['year']\n",
    "    \n",
    "    \n",
    "    #fill up NaNs for `Number_of_holidays` column\n",
    "    #I sorted the values so that the values are ordered by time, and the NaNs are at the end of each time period\n",
    "    df_weather = df_weather.sort_values(['month', 'year', 'Number_of_holidays'])\n",
    "    df_weather['Number_of_holidays'] = df_weather['Number_of_holidays'].fillna(method='ffill')\n",
    "    \n",
    "    #fill up NaNs for the `Green_2016` column\n",
    "    #I only found values for Alaska and North Carolina, so I just put 0 for the other states\n",
    "    df_weather['Green_2016'] = np.where(\n",
    "     df_weather['state'] == 'Alaska', 5735, \n",
    "         np.where(\n",
    "            df_weather['state'] == 'North Carolina', 12105,  \n",
    "             np.where(\n",
    "                df_weather['Green_2016'].isnull(), 0, df_weather['Green_2016'] \n",
    "             )\n",
    "         )\n",
    "    )\n",
    "    \n",
    "    df_weather['climate'] = df_weather['climate'].fillna(df_weather['climate'].mode()[0])\n",
    "    \n",
    "    #Substitute every remaining NaNs by 0\n",
    "    df_weather = df_weather.fillna(value=0)\n",
    "    \n",
    "    output_data = input_data.copy()\n",
    "    output_data.loc[:, 'Democrats_08_Votes':] = df_weather.loc[:, 'Democrats_08_Votes':]\n",
    "    output_data['climate'] = df_weather['climate']\n",
    "    \n",
    "    #Tests\n",
    "    \n",
    "    #Check that there are no missing values in the `Number_of_holidays` column\n",
    "    if not output_data['Number_of_holidays'].isnull().sum() == 0:\n",
    "        raise Error('There should not be NaNs in the Number_of_holidays column')\n",
    "    \n",
    "    #Check that every month has only one value for the `Number_of_holiday` column\n",
    "    number_of_error = 0\n",
    "    for month in range(12):\n",
    "        for year in [2018, 2019]:\n",
    "            sub_df = output_data[(output_data['month'] == month+1) & (output_data['year'] == year)]\n",
    "            if len(sub_df['Number_of_holidays'].unique()) > 1:\n",
    "                number_of_error += 1 \n",
    "    if not number_of_error == 0:\n",
    "        raise Error('Every month should have the same value for Number_of_holidays')\n",
    "               \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_neighbour(input_data):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, preprocess the columns\n",
    "    related to locale information (`city` to\n",
    "    `houses_per_sq_km`). Drop columns with >30%\n",
    "    NaN values and replace remaining NaN values with 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_neighbour = input_data.loc[:, 'city':'houses_per_sq_km']\n",
    "    df_neighbour.drop(columns=['climate'])\n",
    "    missing = df_neighbour.isna()\n",
    "    \n",
    "    # Count number of missing values for each column\n",
    "    num_missing = missing.sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate proportion of missing values for each column\n",
    "    prop_missing = num_missing / df.shape[0]\n",
    "    \n",
    "    # Create a list of columns with >30% of values missing\n",
    "    to_drop = prop_missing[prop_missing > 0.3].index.to_list()\n",
    "    \n",
    "    # Add `country` to the list since all playgrounds are in the U.S.\n",
    "    # Add `city` and `county` since lat. and long. should take care of them\n",
    "    to_drop.append('country')\n",
    "    to_drop.append('city')\n",
    "    to_drop.append('county')\n",
    "    \n",
    "    # Drop columns with names in list\n",
    "    output_data = input_data.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Fill in remaining NaN values in locale-related columns with 0\n",
    "    to_impute = prop_missing[(0 < prop_missing) & (prop_missing <= 0.3)].index.to_list()\n",
    "    to_impute.remove('city')\n",
    "    to_impute.remove('county')\n",
    "    output_data[to_impute] = output_data[to_impute].fillna(0)\n",
    "    output_data['climate'] = input_data['climate']\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_biba(df)\n",
    "data = preprocess_weather(data)\n",
    "data = preprocess_neighbour(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>B20004e10</th>\n",
       "      <th>B11016e1</th>\n",
       "      <th>B12001e12</th>\n",
       "      <th>B20004e11</th>\n",
       "      <th>B19125e1</th>\n",
       "      <th>B12001e13</th>\n",
       "      <th>B23008e22</th>\n",
       "      <th>...</th>\n",
       "      <th>monthly_allages</th>\n",
       "      <th>monthly_travel</th>\n",
       "      <th>monthly_accessible</th>\n",
       "      <th>monthly_Saturday</th>\n",
       "      <th>monthly_Thursday</th>\n",
       "      <th>monthly_Sunday</th>\n",
       "      <th>monthly_Tuesday</th>\n",
       "      <th>monthly_Friday</th>\n",
       "      <th>monthly_Wednesday</th>\n",
       "      <th>monthly_Monday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900203</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900203</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900203</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_id  month  year  B20004e10  B11016e1  B12001e12  B20004e11  \\\n",
       "0     1900203      3  2019      51111      1868        688          0   \n",
       "1     1900203      6  2018      51111      1868        688          0   \n",
       "2     1900203      8  2018      51111      1868        688          0   \n",
       "3  MR00101775      1  2019      45484      2613        980      30417   \n",
       "4  MR00101775      8  2019      45484      2613        980      30417   \n",
       "\n",
       "   B19125e1  B12001e13  B23008e22  ...  monthly_allages  monthly_travel  \\\n",
       "0     78934       1342          0  ...              0.0             0.0   \n",
       "1     78934       1342          0  ...              0.0             0.0   \n",
       "2     78934       1342          0  ...              0.0             0.0   \n",
       "3     45578       1097         66  ...              0.0             0.0   \n",
       "4     45578       1097         66  ...              0.0             0.0   \n",
       "\n",
       "   monthly_accessible  monthly_Saturday  monthly_Thursday  monthly_Sunday  \\\n",
       "0                 0.0               0.0               0.0             0.0   \n",
       "1                 0.0               0.0               0.0             0.0   \n",
       "2                 0.0               0.0               0.0             0.0   \n",
       "3                 0.0               0.0               0.0             0.0   \n",
       "4                 0.0               0.0               0.0             0.0   \n",
       "\n",
       "   monthly_Tuesday  monthly_Friday  monthly_Wednesday  monthly_Monday  \n",
       "0              0.0             0.0                0.0             0.0  \n",
       "1              0.0             0.0                0.0             0.0  \n",
       "2              0.0             0.0                0.0             0.0  \n",
       "3              0.0             0.0                0.0             0.0  \n",
       "4              0.0             0.0                0.0             0.0  \n",
       "\n",
       "[5 rows x 815 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of ouput data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_categorical(input_data, to_drop=['income_class', 'density_class', 'climate']):\n",
    "    \"\"\"\n",
    "    Given the original dataframe, uses One-Hot-Encoding to encode the categorical variables\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pandas.core.frame.DataFrame\n",
    "    to_drop : list\n",
    "        The list of the categorical variables on which we want to apply OHE\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_data : pandas.core.frame.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output_data = input_data.copy()\n",
    "\n",
    "    #Apply One-Hot-Encoding to each one of the categorical variable\n",
    "    for col in to_drop:\n",
    "        ohe = OneHotEncoder(sparse=False, dtype=int)\n",
    "        sub_df = pd.DataFrame(ohe.fit_transform(input_data[[col]]), columns=ohe.categories_[0])\n",
    "        output_data = pd.concat((output_data, sub_df), axis=1)\n",
    "    #Drop the columns for which we used OHE\n",
    "    output_data.drop(columns = to_drop, inplace=True)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on three columns:\n",
    "data = clean_categorical(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>B20004e10</th>\n",
       "      <th>B11016e1</th>\n",
       "      <th>B12001e12</th>\n",
       "      <th>B20004e11</th>\n",
       "      <th>B19125e1</th>\n",
       "      <th>B12001e13</th>\n",
       "      <th>B23008e22</th>\n",
       "      <th>...</th>\n",
       "      <th>monthly_Monday</th>\n",
       "      <th>HI</th>\n",
       "      <th>LI</th>\n",
       "      <th>MI</th>\n",
       "      <th>HD</th>\n",
       "      <th>LD</th>\n",
       "      <th>MD</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900203</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900203</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1900203</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>51111</td>\n",
       "      <td>1868</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>78934</td>\n",
       "      <td>1342</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MR00101775</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>45484</td>\n",
       "      <td>2613</td>\n",
       "      <td>980</td>\n",
       "      <td>30417</td>\n",
       "      <td>45578</td>\n",
       "      <td>1097</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 821 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_id  month  year  B20004e10  B11016e1  B12001e12  B20004e11  \\\n",
       "0     1900203      3  2019      51111      1868        688          0   \n",
       "1     1900203      6  2018      51111      1868        688          0   \n",
       "2     1900203      8  2018      51111      1868        688          0   \n",
       "3  MR00101775      1  2019      45484      2613        980      30417   \n",
       "4  MR00101775      8  2019      45484      2613        980      30417   \n",
       "\n",
       "   B19125e1  B12001e13  B23008e22  ...  monthly_Monday  HI  LI  MI  HD  LD  \\\n",
       "0     78934       1342          0  ...             0.0   1   0   0   1   0   \n",
       "1     78934       1342          0  ...             0.0   1   0   0   1   0   \n",
       "2     78934       1342          0  ...             0.0   1   0   0   1   0   \n",
       "3     45578       1097         66  ...             0.0   0   1   0   0   1   \n",
       "4     45578       1097         66  ...             0.0   0   1   0   0   1   \n",
       "\n",
       "   MD  A  C  D  \n",
       "0   0  0  1  0  \n",
       "1   0  0  1  0  \n",
       "2   0  0  1  0  \n",
       "3   0  0  1  0  \n",
       "4   0  0  1  0  \n",
       "\n",
       "[5 rows x 821 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe after all pre-processing\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('unacast_session_count', axis=1)\n",
    "y = data.loc[:, 'unacast_session_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, drop `external_id` and `state`\n",
    "X = X.drop(['external_id', 'state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "days_since_first_sess    17800\n",
       "D                            0\n",
       "B12001e2                     0\n",
       "B11005e9                     0\n",
       "B13016e5                     0\n",
       "                         ...  \n",
       "precip_mm_10_above           0\n",
       "precip_mm_1_10               0\n",
       "precip_mm_0_1                0\n",
       "precip_mm_none               0\n",
       "month                        0\n",
       "Length: 818, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are missing values in X\n",
    "X.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, fill `days_since_first_sess` with 0\n",
    "X['days_since_first_sess'] = X['days_since_first_sess'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are missing values in y\n",
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `NaN` values in `y` - that's good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40096, 818)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10024, 818)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a `GradientBoostingRegressor` with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'verbose': 1,\n",
    "          'random_state': 2020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      213669.2304            4.86m\n",
      "         2      205282.8246            4.79m\n",
      "         3      198390.9645            4.72m\n",
      "         4      191658.6878            5.01m\n",
      "         5      186589.0004            4.92m\n",
      "         6      181186.5017            4.90m\n",
      "         7      177208.5001            4.84m\n",
      "         8      174242.0719            5.04m\n",
      "         9      171638.3291            5.22m\n",
      "        10      165022.6046            5.32m\n",
      "        20      129466.3909            4.65m\n",
      "        30       99860.7850            3.98m\n",
      "        40       81710.4135            3.35m\n",
      "        50       70320.0408            2.81m\n",
      "        60       62436.7513            2.22m\n",
      "        70       58201.8945            1.67m\n",
      "        80       53252.5016            1.11m\n",
      "        90       49244.3230           33.05s\n",
      "       100       46957.9923            0.00s\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(**params)\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370204.9588049915"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MSE\n",
    "y_pred = gbr.predict(X_valid)\n",
    "mean_squared_error(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27425744561888155"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate R^2 of the prediction\n",
    "gbr.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another `GradientBoostingRegressor` where `n_estimators` is doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1000 = {'verbose': 1,\n",
    "               'n_estimators': 1000,\n",
    "               'random_state': 2020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      213669.2304           48.80m\n",
      "         2      205282.8246           47.63m\n",
      "         3      198390.9645           47.96m\n",
      "         4      191658.6878           49.80m\n",
      "         5      186589.0004           53.81m\n",
      "         6      181186.5017           53.67m\n",
      "         7      177208.5001           53.37m\n",
      "         8      174242.0719           53.11m\n",
      "         9      171638.3291           52.80m\n",
      "        10      165022.6046           52.90m\n",
      "        20      129466.3909           50.69m\n",
      "        30       99860.7850           51.99m\n",
      "        40       81710.4135           51.96m\n",
      "        50       70320.0408           51.28m\n",
      "        60       62436.7513           53.76m\n",
      "        70       58201.8945           53.96m\n",
      "        80       53252.5016           52.57m\n",
      "        90       49244.3230           50.99m\n",
      "       100       46957.9923           49.76m\n",
      "       200       33108.7092           43.56m\n",
      "       300       26537.4012           37.29m\n",
      "       400       22282.0518           32.59m\n",
      "       500       19214.4100           26.89m\n",
      "       600       16629.4686           21.27m\n",
      "       700       14954.9852           15.76m\n",
      "       800       13701.1824           10.43m\n",
      "       900       12449.9097            5.17m\n",
      "      1000       11590.5045            0.00s\n"
     ]
    }
   ],
   "source": [
    "gbr_1000 = GradientBoostingRegressor(**params_1000)\n",
    "\n",
    "t0 = time.time()\n",
    "gbr_1000.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "fit_time = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364149.91425749223"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MSE\n",
    "y_pred_1000 = gbr_1000.predict(X_valid)\n",
    "mean_squared_error(y_valid, y_pred_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2861276364206973"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate R^2 of the prediction\n",
    "gbr_1000.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform randomized search of optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'min_samples_split': [2, 4, 6],\n",
    "              'max_depth': [3, 5, 7, 9],\n",
    "              'max_features': ['auto', 'sqrt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=3 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      153317.5171           65.14m\n",
      "         2      142882.3344           66.35m\n",
      "         3      135685.8230           66.56m\n",
      "         4      128130.1997           66.66m\n",
      "         5      122470.8207           66.94m\n",
      "         6      118451.6037           66.95m\n",
      "         7      115259.8875           67.29m\n",
      "         8      112609.5008           67.27m\n",
      "         9      109552.1000           67.53m\n",
      "        10      107623.1704           68.16m\n",
      "        20       89430.5067           67.99m\n",
      "        30       71334.1225           69.04m\n",
      "        40       60044.3450           69.81m\n",
      "        50       52779.6393           69.76m\n",
      "        60       47767.4358           69.39m\n",
      "        70       43246.5435           68.77m\n",
      "        80       40850.4885           68.26m\n",
      "        90       39042.1967           67.94m\n",
      "       100       37326.2071           67.25m\n",
      "       200       24353.1053           53.62m\n",
      "       300       17797.7181           38.50m\n",
      "       400       14449.5789           29.30m\n",
      "       500       12265.7430           22.57m\n",
      "       600       10449.3770           17.05m\n",
      "       700        9320.8869           12.27m\n",
      "       800        8350.0983            7.91m\n",
      "       900        7601.7513            3.85m\n",
      "      1000        6869.0822            0.00s\n",
      "[CV]  min_samples_split=4, max_features=auto, max_depth=3, total=37.7min\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=3 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 37.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      241967.4629           30.65m\n",
      "         2      225379.2903           30.43m\n",
      "         3      208913.0207           30.43m\n",
      "         4      201653.9873           30.33m\n",
      "         5      188521.9523           30.30m\n",
      "         6      177523.3656           30.27m\n",
      "         7      166659.6284           30.17m\n",
      "         8      158616.3034           30.14m\n",
      "         9      150653.1653           30.08m\n",
      "        10      143361.2094           30.05m\n",
      "        20      107967.1941           29.69m\n",
      "        30       81686.2252           29.74m\n",
      "        40       65468.1422           29.55m\n",
      "        50       54354.3595           29.37m\n",
      "        60       48476.4330           29.05m\n",
      "        70       45189.1124           28.73m\n",
      "        80       42161.0303           28.38m\n",
      "        90       39027.2504           28.08m\n",
      "       100       36137.7814           27.76m\n",
      "       200       25283.7676           24.63m\n",
      "       300       19759.4666           21.70m\n",
      "       400       16222.4392           19.32m\n",
      "       500       13572.0073           16.52m\n",
      "       600       11969.1479           13.55m\n",
      "       700       10306.1348           10.32m\n",
      "       800        9152.2162            6.94m\n",
      "       900        8301.7948            3.48m\n",
      "      1000        7510.6889            0.00s\n",
      "[CV]  min_samples_split=4, max_features=auto, max_depth=3, total=35.0min\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=3 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      240221.6672           44.82m\n",
      "         2      224642.3947           41.35m\n",
      "         3      208393.6742           42.52m\n",
      "         4      195086.3002           42.13m\n",
      "         5      184528.7218           40.95m\n",
      "         6      174314.2392           40.92m\n",
      "         7      165901.3321           40.43m\n",
      "         8      158411.5076           39.76m\n",
      "         9      151906.4892           39.77m\n",
      "        10      145844.8002           39.56m\n",
      "        20      113127.1241           37.26m\n",
      "        30       90269.7455           36.57m\n",
      "        40       71186.1487           38.46m\n",
      "        50       59605.9486           37.83m\n",
      "        60       52526.2487           36.92m\n",
      "        70       48008.5433           36.05m\n",
      "        80       44975.7400           35.21m\n",
      "        90       42358.3639           34.94m\n",
      "       100       40096.6931           34.23m\n",
      "       200       26770.2200           29.14m\n",
      "       300       20641.3234           25.08m\n",
      "       400       15950.1855           21.36m\n",
      "       500       13158.8593           17.69m\n",
      "       600       11330.2752           14.12m\n",
      "       700        9777.5656           10.58m\n",
      "       800        8645.2277            7.07m\n",
      "       900        7781.1475            3.52m\n",
      "      1000        7185.2461            0.00s\n",
      "[CV]  min_samples_split=4, max_features=auto, max_depth=3, total=35.3min\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=7 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      139955.9308           84.32m\n",
      "         2      120636.6704           84.45m\n",
      "         3      104240.9834           84.12m\n",
      "         4       90661.3218           83.53m\n",
      "         5       79268.6824           83.62m\n",
      "         6       69584.5782           82.60m\n",
      "         7       61991.0093           81.86m\n",
      "         8       55675.0023           82.04m\n",
      "         9       50182.8420           81.37m\n",
      "        10       46064.5668           80.75m\n",
      "        20       21958.1132           80.26m\n",
      "        30       13884.9788           79.26m\n",
      "        40        9628.0516           79.44m\n",
      "        50        7428.5534           79.19m\n",
      "        60        6069.7875           78.15m\n",
      "        70        5313.4477           77.64m\n",
      "        80        4534.8533           76.82m\n",
      "        90        4101.8099           75.53m\n",
      "       100        3717.4627           74.53m\n",
      "       200        1781.1446           65.72m\n",
      "       300        1113.2378           57.46m\n",
      "       400         794.4664           49.43m\n",
      "       500         585.5602           41.22m\n",
      "       600         441.9735           33.44m\n",
      "       700         333.2927           25.51m\n",
      "       800         258.9561           17.14m\n",
      "       900         203.2628            8.81m\n",
      "      1000         158.7878            0.00s\n",
      "[CV]  min_samples_split=4, max_features=auto, max_depth=7, total=88.6min\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=7 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      221142.7151           85.23m\n",
      "         2      188321.1186           86.27m\n",
      "         3      161288.8493           85.65m\n",
      "         4      137749.2520           85.74m\n",
      "         5      118405.1042           87.19m\n",
      "         6      102776.3310           86.48m\n",
      "         7       90214.6173           85.99m\n",
      "         8       79317.8171           85.58m\n",
      "         9       69643.8235           85.49m\n",
      "        10       62662.3745           85.34m\n",
      "        20       26946.2717           86.15m\n",
      "        30       15279.0365           86.73m\n",
      "        40       10950.5773           85.72m\n",
      "        50        8290.1032           84.94m\n",
      "        60        6828.1246           83.82m\n",
      "        70        5755.4320           82.85m\n",
      "        80        4961.6475           81.68m\n",
      "        90        4296.6196           81.21m\n",
      "       100        3741.8642           80.14m\n",
      "       200        1683.7851           71.29m\n",
      "       300        1079.6167           62.00m\n",
      "       400         761.8538           53.95m\n",
      "       500         563.4655           45.14m\n",
      "       600         429.0603           36.27m\n",
      "       700         322.4795           27.36m\n",
      "       800         247.1127           18.22m\n",
      "       900         189.9890            8.93m\n",
      "      1000         144.3119            0.00s\n",
      "[CV]  min_samples_split=4, max_features=auto, max_depth=7, total=87.8min\n",
      "[CV] min_samples_split=4, max_features=auto, max_depth=7 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      219337.4996           72.52m\n",
      "         2      187813.4630           71.17m\n",
      "         3      160560.0152           71.20m\n",
      "         4      138910.8902           70.88m\n",
      "         5      121151.1839           70.63m\n",
      "         6      106756.6710           70.38m\n",
      "         7       94685.5184           70.26m\n",
      "         8       85830.1229           70.12m\n",
      "         9       79068.4560           70.00m\n",
      "        10       71525.6090           70.01m\n",
      "        20       31330.0010           70.51m\n",
      "        30       18771.5650           70.36m\n",
      "        40       12221.6774           70.15m\n",
      "        50        9320.1417           69.52m\n",
      "        60        7638.0619           68.79m\n",
      "        70        6463.8768           67.84m\n",
      "        80        5568.1654           67.05m\n",
      "        90        4848.7561           66.32m\n",
      "       100        4254.0861           65.50m\n",
      "       200        1987.4639           57.77m\n",
      "       300        1264.6727           50.53m\n",
      "       400         899.7452           43.38m\n",
      "       500         660.8410           36.20m\n",
      "       600         494.1790           29.01m\n",
      "       700         371.9100           21.81m\n",
      "       800         288.3596           14.56m\n",
      "       900         221.3388            7.29m\n",
      "      1000         174.6572            0.00s\n",
      "[CV]  min_samples_split=4, max_features=auto, max_depth=7, total=73.0min\n",
      "[CV] min_samples_split=2, max_features=sqrt, max_depth=7 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      147415.2818            2.46m\n",
      "         2      138723.6092            2.50m\n",
      "         3      128922.2113            2.49m\n",
      "         4      119980.1320            2.49m\n",
      "         5      107417.6587            2.47m\n",
      "         6      100148.7136            2.47m\n",
      "         7       93359.2805            2.46m\n",
      "         8       87320.6982            2.44m\n",
      "         9       78627.5706            2.43m\n",
      "        10       72879.7611            2.42m\n",
      "        20       43247.0196            2.42m\n",
      "        30       27591.8990            2.44m\n",
      "        40       19660.0309            2.45m\n",
      "        50       15924.5838            2.42m\n",
      "        60       12676.6009            2.40m\n",
      "        70       10229.0150            2.37m\n",
      "        80        8787.4725            2.35m\n",
      "        90        7663.6492            2.32m\n",
      "       100        6963.4093            2.29m\n",
      "       200        3006.9501            2.05m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       300        1952.7888            1.80m\n",
      "       400        1389.6867            1.54m\n",
      "       500        1055.7018            1.29m\n",
      "       600         842.7764            1.03m\n",
      "       700         665.2847           46.73s\n",
      "       800         535.4185           31.21s\n",
      "       900         448.4783           15.64s\n",
      "      1000         372.9179            0.00s\n",
      "[CV]  min_samples_split=2, max_features=sqrt, max_depth=7, total= 2.6min\n",
      "[CV] min_samples_split=2, max_features=sqrt, max_depth=7 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      231119.0006            2.55m\n",
      "         2      206885.3001            2.52m\n",
      "         3      186561.3483            2.49m\n",
      "         4      162252.3419            2.50m\n",
      "         5      141641.5785            2.48m\n",
      "         6      126785.6327            2.48m\n",
      "         7      113329.5210            2.46m\n",
      "         8      101487.7961            2.46m\n",
      "         9       89886.4764            2.46m\n",
      "        10       80677.5757            2.45m\n",
      "        20       44273.4174            2.44m\n",
      "        30       27760.7878            2.44m\n",
      "        40       19203.5563            2.43m\n",
      "        50       15409.9478            2.41m\n",
      "        60       12696.6497            2.39m\n",
      "        70       10651.1273            2.36m\n",
      "        80        8998.4862            2.34m\n",
      "        90        7857.3781            2.31m\n",
      "       100        6883.2789            2.28m\n",
      "       200        2939.9834            2.04m\n",
      "       300        1840.2047            1.79m\n",
      "       400        1345.2018            1.54m\n",
      "       500        1030.6132            1.29m\n",
      "       600         812.9916            1.03m\n",
      "       700         654.0344           46.70s\n",
      "       800         535.2096           31.21s\n",
      "       900         438.4678           15.64s\n",
      "      1000         358.7933            0.00s\n",
      "[CV]  min_samples_split=2, max_features=sqrt, max_depth=7, total= 2.6min\n",
      "[CV] min_samples_split=2, max_features=sqrt, max_depth=7 .............\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      231426.4103            2.43m\n",
      "         2      204700.7992            2.48m\n",
      "         3      182596.2553            2.47m\n",
      "         4      162682.1391            2.49m\n",
      "         5      145980.7143            2.48m\n",
      "         6      128991.0876            2.48m\n",
      "         7      112095.2661            2.46m\n",
      "         8      103386.3814            2.46m\n",
      "         9       92134.5792            2.45m\n",
      "        10       86089.5504            2.45m\n",
      "        20       42530.8964            2.44m\n",
      "        30       26644.7065            2.45m\n",
      "        40       18893.7696            2.45m\n",
      "        50       15023.3580            2.42m\n",
      "        60       12278.2040            2.40m\n",
      "        70       10294.5594            2.37m\n",
      "        80        9044.3196            2.34m\n",
      "        90        7898.5929            2.31m\n",
      "       100        6874.3283            2.29m\n",
      "       200        3118.7809            2.05m\n",
      "       300        1988.4015            1.80m\n",
      "       400        1423.4316            1.54m\n",
      "       500        1072.2048            1.29m\n",
      "       600         831.9145            1.04m\n",
      "       700         665.2816           46.69s\n",
      "       800         539.9810           31.25s\n",
      "       900         439.5051           15.66s\n",
      "      1000         361.9735            0.00s\n",
      "[CV]  min_samples_split=2, max_features=sqrt, max_depth=7, total= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 365.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      201432.3232            3.71m\n",
      "         2      187698.6130            3.69m\n",
      "         3      172924.2830            3.59m\n",
      "         4      165930.4164            3.59m\n",
      "         5      152832.8956            3.57m\n",
      "         6      137973.6947            3.56m\n",
      "         7      129007.9530            3.54m\n",
      "         8      119984.4184            3.54m\n",
      "         9      110339.4638            3.54m\n",
      "        10      100848.2244            3.51m\n",
      "        20       54586.3249            3.48m\n",
      "        30       35933.6346            3.48m\n",
      "        40       25765.4375            3.47m\n",
      "        50       19896.6681            3.44m\n",
      "        60       15900.4821            3.40m\n",
      "        70       13601.7552            3.39m\n",
      "        80       11404.3946            3.36m\n",
      "        90       10302.6814            3.33m\n",
      "       100        9233.4724            3.29m\n",
      "       200        4295.5839            2.94m\n",
      "       300        2765.3633            2.57m\n",
      "       400        2028.5001            2.21m\n",
      "       500        1602.7798            1.85m\n",
      "       600        1308.7908            1.48m\n",
      "       700        1064.1225            1.11m\n",
      "       800         896.4873           44.64s\n",
      "       900         767.9447           22.37s\n",
      "      1000         651.2235            0.00s\n"
     ]
    }
   ],
   "source": [
    "gbr_gs = GradientBoostingRegressor(n_estimators=1000, verbose=1, random_state=2020)\n",
    "\n",
    "rscv = RandomizedSearchCV(gbr_gs, param_grid, n_iter=3, verbose=2, cv=3, n_jobs=1, random_state=2020)\n",
    "\n",
    "search = rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 7}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the most optimal hyperparameter settings\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2159.919200</td>\n",
       "      <td>73.135515</td>\n",
       "      <td>1.049887</td>\n",
       "      <td>0.263267</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_split': 4, 'max_features': 'auto...</td>\n",
       "      <td>0.161272</td>\n",
       "      <td>0.364089</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>0.193555</td>\n",
       "      <td>0.128111</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4987.493293</td>\n",
       "      <td>430.139624</td>\n",
       "      <td>1.645919</td>\n",
       "      <td>0.316282</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>7</td>\n",
       "      <td>{'min_samples_split': 4, 'max_features': 'auto...</td>\n",
       "      <td>0.190472</td>\n",
       "      <td>0.284677</td>\n",
       "      <td>0.402996</td>\n",
       "      <td>0.292715</td>\n",
       "      <td>0.086948</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157.151442</td>\n",
       "      <td>0.092279</td>\n",
       "      <td>1.505860</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>7</td>\n",
       "      <td>{'min_samples_split': 2, 'max_features': 'sqrt...</td>\n",
       "      <td>0.128976</td>\n",
       "      <td>0.545482</td>\n",
       "      <td>0.384348</td>\n",
       "      <td>0.352935</td>\n",
       "      <td>0.171483</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0    2159.919200     73.135515         1.049887        0.263267   \n",
       "1    4987.493293    430.139624         1.645919        0.316282   \n",
       "2     157.151442      0.092279         1.505860        0.004716   \n",
       "\n",
       "  param_min_samples_split param_max_features param_max_depth  \\\n",
       "0                       4               auto               3   \n",
       "1                       4               auto               7   \n",
       "2                       2               sqrt               7   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'min_samples_split': 4, 'max_features': 'auto...           0.161272   \n",
       "1  {'min_samples_split': 4, 'max_features': 'auto...           0.190472   \n",
       "2  {'min_samples_split': 2, 'max_features': 'sqrt...           0.128976   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.364089           0.055305         0.193555        0.128111   \n",
       "1           0.284677           0.402996         0.292715        0.086948   \n",
       "2           0.545482           0.384348         0.352935        0.171483   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                2  \n",
       "2                1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the evaluation of the hyperparameter candidates\n",
    "result = search.cv_results_\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651.2234780126952"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call predict on the most optimal hyperparameter on the training set\n",
    "# Calculate MSE\n",
    "mean_squared_error(y_train, search.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971445104080854"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate R^2 of the prediction on the training set\n",
    "search.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394161.808067091"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call predict on the most optimal hyperparameter on the validation set\n",
    "# Calculate MSE\n",
    "mean_squared_error(y_valid, search.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2272929072871356"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate R^2 of the prediction on the validation set\n",
    "search.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try removing the playgrounds with over 100,000 lifetime sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "- http://www.chengli.io/tutorials/gradient_boosting.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
